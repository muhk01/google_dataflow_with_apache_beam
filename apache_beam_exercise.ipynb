{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d3e42d-9412-4667-9617-0dfba30114a3",
   "metadata": {},
   "source": [
    "# Apache Beam Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0644c9d-4743-49b1-a889-c39c303ec7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524634ab-30bf-4772-abe3-74741b9482c7",
   "metadata": {},
   "source": [
    "# Transformation in Apache Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb320634-5c2d-4f97-a811-46f0540472db",
   "metadata": {},
   "source": [
    "## Sample Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c55f409e-9cf9-4310-9108-43d9b6319167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x2344dff7548>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = beam.Pipeline()\n",
    "attendance_count = (\n",
    "    p1\n",
    "    | beam.io.ReadFromText('./testdata/beam_data/dept_data.txt')\n",
    "    | beam.Map(lambda record: record.split(','))\n",
    "    | beam.Filter(lambda record: record[3] == 'Accounts')\n",
    "    | beam.Map(lambda record: (record[1], 1))\n",
    "    | beam.CombinePerKey(sum)\n",
    "    | beam.Map(lambda record: str(record))\n",
    "    | beam.io.WriteToText('./beamoutput/out')\n",
    ")\n",
    "p1.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7c726-315a-4485-aa49-b9969d7c03f7",
   "metadata": {},
   "source": [
    "Checking output since I am running on Windows env cannot execute such unix command like bash/head/ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3a3040d9-4526-4897-b00e-84e3f48bb59a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marco', 31)\n",
      "('Rebekah', 31)\n",
      "('Itoe', 31)\n",
      "('Edouard', 31)\n",
      "('Kyle', 62)\n"
     ]
    }
   ],
   "source": [
    "!powershell -Command \"Get-Content -TotalCount 5 './beamoutput/out-00000-of-00001'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d95680f-2dda-444b-8ee2-3166a1c03383",
   "metadata": {},
   "source": [
    "## Example P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4ebd08f-65b4-46f7-9049-81a51281bf27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x2344e08f308>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = beam.Pipeline()\n",
    "lines = (\n",
    "    p2\n",
    "    | beam.Create(['Line1',\n",
    "                  'Line2',\n",
    "                  'Line3',\n",
    "                  'Line4'])\n",
    "    | beam.io.WriteToText('./beamoutput/outCreate')\n",
    ")\n",
    "\n",
    "p2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4de391a7-5eb3-4b39-80fa-39f74c1ed440",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line1\n",
      "Line2\n",
      "Line3\n",
      "Line4\n"
     ]
    }
   ],
   "source": [
    "!powershell -Command \"Get-Content -TotalCount 5 './beamoutput/outCreate-00000-of-00001'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cadd14-e922-4591-ab95-5e646e873e57",
   "metadata": {},
   "source": [
    "## Example P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7b9c69a-a154-49d8-9894-1dfe5d527fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x2344e0a7b48>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3 = beam.Pipeline()\n",
    "lines = (\n",
    "    p3\n",
    "    | beam.Create([('Line1',52),\n",
    "                  ('Line2',75),\n",
    "                  ('Line3',82),\n",
    "                  ('Line4',65)])\n",
    "    | beam.io.WriteToText('./beamoutput/outCreate1')\n",
    ")\n",
    "\n",
    "p3.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "79d2022a-94bc-43fe-9def-e17dd8c69215",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Line1', 52)\n",
      "('Line2', 75)\n",
      "('Line3', 82)\n",
      "('Line4', 65)\n"
     ]
    }
   ],
   "source": [
    "!powershell -Command \"Get-Content -TotalCount 5 './beamoutput/outCreate1-00000-of-00001'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24993370-e004-496b-8b28-da2bd38b2d77",
   "metadata": {},
   "source": [
    "## Map Function and User Defined Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c847b-4172-4df3-88d4-9ccf1104ae23",
   "metadata": {},
   "source": [
    "In Apache Beam, the Map transform is used to apply a user-defined function to each element in a PCollection (a collection of data) and produce a new PCollection with the results. It is a fundamental transform that allows you to perform element-wise transformations on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da2e7382-32c6-4777-ad18-9cabf041d585",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Line1', 52)\n",
      "('Line2', 75)\n",
      "('Line3', 82)\n",
      "('Line4', 65)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x2344fbb4208>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p4 = beam.Pipeline()\n",
    "lines = (\n",
    "    p4\n",
    "    | beam.Create([('Line1', 52),\n",
    "                   ('Line2', 75),\n",
    "                   ('Line3', 82),\n",
    "                   ('Line4', 65)])\n",
    "    | beam.Map(print)\n",
    ")\n",
    "p4.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627069fd-2ac3-4732-ba92-9bbba25b8267",
   "metadata": {},
   "source": [
    "Now we can use it to output without doing bash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ff0c04a-7c2f-42d6-8fa5-703aa77c6572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiply_by_two(x):\n",
    "    return x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecd6436a-f75e-4443-b477-5163e6d49719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x2344f9e89c8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p5 = beam.Pipeline()\n",
    "numbers = (\n",
    "    p5\n",
    "    | beam.Create([1, 2, 3, 4, 5])\n",
    "    | beam.Map(multiply_by_two)\n",
    "    | beam.Map(print)\n",
    ")\n",
    "p5.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350a38f-99d7-49e9-8efb-0d963af82b75",
   "metadata": {},
   "source": [
    "in Here udf or user defined function received from Map and apply for each rows it has, then pipeline printing its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47b1bd12-ee25-457f-90a7-9774fb4456d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def splitRow(element):\n",
    "    return element.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d54793c-81a1-400a-b2b2-a81e2e87da16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filterRow(element):\n",
    "    return element[3] == 'Accounts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05efdb62-8e0b-4638-86a5-00b31f9c2734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pairKey(element):\n",
    "    return (element[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1bcf478-1c86-4e06-a5ad-5af5513e2f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marco', 31)\n",
      "('Rebekah', 31)\n",
      "('Itoe', 31)\n",
      "('Edouard', 31)\n",
      "('Kyle', 62)\n",
      "('Kumiko', 31)\n",
      "('Gaston', 31)\n",
      "('Ayumi', 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x2344d5f6f08>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p6 = beam.Pipeline()\n",
    "splitCols = (\n",
    "    p6\n",
    "    | beam.io.ReadFromText('./testdata/beam_data/dept_data.txt')\n",
    "    | beam.Map(splitRow)\n",
    "    | beam.Filter(filterRow)\n",
    "    | beam.Map(pairKey)\n",
    "    | beam.CombinePerKey(sum)\n",
    "    | beam.Map(print)\n",
    ")\n",
    "p6.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683db70-4f96-45a6-a8ac-f87e1e992767",
   "metadata": {},
   "source": [
    "## Using with to initiate Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e6ea53d-95c6-48eb-888c-1a95f7dd9359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marco', 31)\n",
      "('Rebekah', 31)\n",
      "('Itoe', 31)\n",
      "('Edouard', 31)\n",
      "('Kyle', 62)\n",
      "('Kumiko', 31)\n",
      "('Gaston', 31)\n",
      "('Ayumi', 30)\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p7:\n",
    "    splitCols = (\n",
    "        p7\n",
    "        | beam.io.ReadFromText('./testdata/beam_data/dept_data.txt')\n",
    "        | beam.Map(splitRow)\n",
    "        | beam.Filter(filterRow)\n",
    "        | beam.Map(pairKey)\n",
    "        | beam.CombinePerKey(sum)\n",
    "        | beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc84da-8efa-408b-adb8-b5a7ea1cf879",
   "metadata": {},
   "source": [
    "Using with we don't have to run pipeline with method *run()*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d7e33-86fe-46cf-88f5-43b173b231ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Brancing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3887058c-6642-4f9c-bd08-9024b9c4da5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ACC, Marco', 31)\n",
      "('ACC, Rebekah', 31)\n",
      "('ACC, Itoe', 31)\n",
      "('ACC, Edouard', 31)\n",
      "('ACC, Kyle', 62)\n",
      "('ACC, Kumiko', 31)\n",
      "('ACC, Gaston', 31)\n",
      "('ACC, Ayumi', 30)\n",
      "('HR, Beryl', 62)\n",
      "('HR, Olga', 31)\n",
      "('HR, Leslie', 31)\n",
      "('HR, Mindy', 31)\n",
      "('HR, Vicky', 31)\n",
      "('HR, Richard', 31)\n",
      "('HR, Kirk', 31)\n",
      "('HR, Kaori', 31)\n",
      "('HR, Oscar', 31)\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p8:\n",
    "    read_input = (\n",
    "        p8\n",
    "        | \"Read Input\" >> beam.io.ReadFromText('./testdata/beam_data/dept_data.txt')\n",
    "        | \"Split Row\"  >> beam.Map(lambda record: record.split(','))\n",
    "    )\n",
    "    accounts_transform = (\n",
    "        read_input\n",
    "        | \"Get Dept Accounts\" >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
    "        | \"Pair Acc Emp with 1\" >> beam.Map(lambda record: (\"ACC, \" + record[1], 1))\n",
    "        | \"Group and Sum\" >> beam.CombinePerKey(sum)\n",
    "        | \"Print acc result\" >> beam.Map(print)\n",
    "    )\n",
    "    hr_transform = (\n",
    "        read_input\n",
    "        | \"Get Dept HR\" >> beam.Filter(lambda record: record[3] == 'HR')\n",
    "        | \"Pair HR Emp with 1\" >> beam.Map(lambda record: (\"HR, \" + record[1], 1))\n",
    "        | \"Group and Sum2\" >> beam.CombinePerKey(sum)\n",
    "        | \"Print HR result\" >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc5ee3-cae0-40ff-8bd8-a7cb1306bcff",
   "metadata": {},
   "source": [
    "## ParDo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24fb20-44f5-4eec-b543-3cf54e825b12",
   "metadata": {},
   "source": [
    "ParDo is a fundamental transform in Apache Beam that allows you to apply a user-defined function (a DoFn) to each element of a PCollection. The name \"ParDo\" stands for \"parallel Do\" and signifies that the function can be executed in parallel across multiple workers or processing units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ab8ccf14-ed76-4ec1-a69c-7e0b51dc9e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class CountWordOccurrences(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        # Extract words using regex\n",
    "        words = re.findall(r'\\b\\w+\\b', element)\n",
    "\n",
    "        # Emit word-count pairs\n",
    "        for word in words:\n",
    "            yield word.lower(), 1\n",
    "\n",
    "with beam.Pipeline() as p9:\n",
    "    Readlines = (\n",
    "        p9\n",
    "        | beam.io.ReadFromText(\"./testdata/beam_data/data.txt\")\n",
    "    )\n",
    "    word_counts = (\n",
    "        Readlines\n",
    "        | beam.ParDo(CountWordOccurrences())\n",
    "        | beam.combiners.Count.PerKey()\n",
    "        | beam.io.WriteToText('./beamoutput/wordCount')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aefb79d5-66ed-42bf-812e-807c99cbd711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('king', 311)\n",
      "('lear', 257)\n",
      "('dramatis', 1)\n",
      "('personae', 1)\n",
      "('of', 483)\n"
     ]
    }
   ],
   "source": [
    "!powershell -Command \"Get-Content -TotalCount 5 './beamoutput/wordCount-00000-of-00001'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466944f-dded-42e8-b76f-d9e03cb6f22e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ParDo #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731bbd4d-a95b-49ba-98fe-f65f88a20bb8",
   "metadata": {},
   "source": [
    "for classes inside the function should be named exactly process in order to override the method from the beam.DoFn class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0ed2ffb8-a6df-4ce1-8da2-f5e09149659e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marco,Accounts', 31)\n",
      "('Rebekah,Accounts', 31)\n",
      "('Itoe,Accounts', 31)\n",
      "('Edouard,Accounts', 31)\n",
      "('Kyle,Accounts', 62)\n",
      "('Kumiko,Accounts', 31)\n",
      "('Gaston,Accounts', 31)\n",
      "('Ayumi,Accounts', 30)\n"
     ]
    }
   ],
   "source": [
    "class splitRowClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        return [element.split(',')]\n",
    "\n",
    "class accountFilterClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element[3] == 'Accounts':\n",
    "            return [element]\n",
    "\n",
    "class pairEmployeeClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "            return [(element[1]+\",\"+element[3],1)]\n",
    "\n",
    "class countingClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "            (key, values) = element\n",
    "            return [(key, sum(values))]\n",
    "\n",
    "with beam.Pipeline() as p10:\n",
    "    readlines = (\n",
    "        p10\n",
    "        | beam.io.ReadFromText('./testdata/beam_data/dept_data.txt')\n",
    "        | beam.ParDo(splitRowClass())\n",
    "        | beam.ParDo(accountFilterClass())\n",
    "        | beam.ParDo(pairEmployeeClass())\n",
    "        | beam.GroupByKey()\n",
    "        | beam.ParDo(countingClass())\n",
    "        | beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6341ce16-a52f-457e-ae68-55518aadfaef",
   "metadata": {},
   "source": [
    "## Combiner (Mini Reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210890fd-55d6-4b83-a4a9-2a792997f5bc",
   "metadata": {},
   "source": [
    "The CombineGlobally transform can be useful in scenarios where you need to compute a summary or aggregate value for the entire dataset, such as calculating the total sum, average, maximum, or any other aggregation that requires considering all elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71acaf90-6e70-4379-b397-7378bfddd36a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5\n"
     ]
    }
   ],
   "source": [
    "class AverageCombiner(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        return (0.0, 0) #(sum, count)\n",
    "\n",
    "    def add_input(self, accumulator, element):\n",
    "        sum_, count = accumulator\n",
    "        sum_of_element = sum_ + element\n",
    "        count_of_element = count + 1\n",
    "        return sum_of_element, count_of_element\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        sums, counts = zip(*accumulators)\n",
    "        return sum(sums), sum(counts)\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        sum_, count = accumulator\n",
    "        return sum_ / count if count != 0 else float('NaN')\n",
    "\n",
    "with beam.Pipeline() as p11:\n",
    "    average = (\n",
    "        p11\n",
    "        | beam.Create([15, 5, 7, 7, 9, 23, 13, 5])\n",
    "        | beam.CombineGlobally(AverageCombiner())\n",
    "        | beam.Map(print)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598148dc-e93c-459c-b364-4a4128c5d700",
   "metadata": {},
   "source": [
    "create_accumulator initializes the accumulator as (0.0, 0).\n",
    "** add_input is called for each element:\n",
    "\n",
    "First element 15: Accumulator becomes (15.0, 1).\n",
    "Second element 5: Accumulator becomes (20.0, 2).\n",
    "Third element 7: Accumulator becomes (27.0, 3).\n",
    "Fourth element 7: Accumulator becomes (34.0, 4).\n",
    "Fifth element 9: Accumulator becomes (43.0, 5).\n",
    "Sixth element 23: Accumulator becomes (66.0, 6).\n",
    "Seventh element 13: Accumulator becomes (79.0, 7).\n",
    "Eighth element 5: Accumulator becomes (84.0, 8).\n",
    "\n",
    "merge_accumulators is not required in this example, as we are not merging accumulators from different workers.\n",
    "\n",
    "extract_output is called to compute the average:\n",
    "The sum in the accumulator is 84.0, and the count is 8.\n",
    "The average is 84.0 / 8 = 10.5.\n",
    "The resulting average, 10.5, is printed as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74577307-d3a0-4e1f-908e-e32520f2fec8",
   "metadata": {},
   "source": [
    "in Apache Beam, the combiner functions are designed to work efficiently in distributed and parallel processing scenarios. The structure of the combiner functions allows for efficient parallel aggregation across multiple workers while handling large datasets.\n",
    "\n",
    "The CombineFn interface in Apache Beam provides a flexible framework for defining the logic of combiner functions. It breaks down the aggregation process into several methods:\n",
    "\n",
    "** create_accumulator: Initializes the accumulator, which can include any necessary state for the aggregation.\n",
    "\n",
    "** add_input: Combines an element with the current accumulator.\n",
    "\n",
    "** merge_accumulators: Combines multiple accumulators together.\n",
    "\n",
    "** extract_output: Extracts the final result from the accumulator.\n",
    "\n",
    "By following this structure, Apache Beam can distribute and parallelize the aggregation process effectively, even for large-scale data processing.\n",
    "\n",
    "If you choose not to implement a combiner and instead use a normal function for aggregation in Apache Beam, it can have several implications can lead to increased network overhead, reduced parallelism, higher memory usage, longer execution times, and scalability challenges in Apache Beam. Implementing a combiner helps mitigate these issues by enabling efficient partial aggregations, reduced network traffic, and better resource utilization, resulting in improved performance and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c83982-133e-4ccd-a82f-613ce6e461f3",
   "metadata": {},
   "source": [
    "# Composite Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7b812-be5f-4a95-a075-965e27ad18b5",
   "metadata": {},
   "source": [
    "In Apache Beam, a composite transform is a higher-level transform that combines multiple primitive transforms into a single, reusable transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "270ac941-6d7f-4bc6-86f0-d5c830ffdf48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'ACC, Marco', '31', 'Regular employee')\n",
      "(b'ACC, Rebekah', '31', 'Regular employee')\n",
      "(b'ACC, Itoe', '31', 'Regular employee')\n",
      "(b'ACC, Edouard', '31', 'Regular employee')\n",
      "(b'ACC, Kyle', '62', 'Regular employee')\n",
      "(b'ACC, Kumiko', '31', 'Regular employee')\n",
      "(b'ACC, Gaston', '31', 'Regular employee')\n",
      "(b'HR, Beryl', '62', 'Regular employee')\n",
      "(b'HR, Olga', '31', 'Regular employee')\n",
      "(b'HR, Leslie', '31', 'Regular employee')\n",
      "(b'HR, Mindy', '31', 'Regular employee')\n",
      "(b'HR, Vicky', '31', 'Regular employee')\n",
      "(b'HR, Richard', '31', 'Regular employee')\n",
      "(b'HR, Kirk', '31', 'Regular employee')\n",
      "(b'HR, Kaori', '31', 'Regular employee')\n",
      "(b'HR, Oscar', '31', 'Regular employee')\n"
     ]
    }
   ],
   "source": [
    "class myTransform(beam.PTransform):\n",
    "    def expand(self, input_col):\n",
    "        a = (\n",
    "            input_col\n",
    "            | beam.CombinePerKey(sum)\n",
    "            | beam.Filter(filter_on_count)\n",
    "            | beam.Map(format_output)\n",
    "            | beam.Map(print)\n",
    "        )\n",
    "        return a #must have return incase in main pipeline has additional processing\n",
    "\n",
    "def filter_on_count(element):\n",
    "    name, count = element\n",
    "    if count > 30:\n",
    "        return element\n",
    "    \n",
    "def format_output(element):\n",
    "    name, count = element\n",
    "    return ((name.encode('ascii'),str(count),'Regular employee'))\n",
    "\n",
    "with beam.Pipeline() as p12:\n",
    "    read_input = (\n",
    "        p12\n",
    "        | \"Read Input\" >> beam.io.ReadFromText('./testdata/beam_data/dept_data.txt')\n",
    "        | \"Split Row\"  >> beam.Map(lambda record: record.split(','))\n",
    "    )\n",
    "    accounts_transform = (\n",
    "        read_input\n",
    "        | \"Get Dept Accounts\" >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
    "        | \"Pair Acc Emp with 1\" >> beam.Map(lambda record: (\"ACC, \" + record[1], 1))\n",
    "        | \"composite Transform acc\" >> myTransform()\n",
    "    )\n",
    "    hr_transform = (\n",
    "        read_input\n",
    "        | \"Get Dept HR\" >> beam.Filter(lambda record: record[3] == 'HR')\n",
    "        | \"Pair HR Emp with 1\" >> beam.Map(lambda record: (\"HR, \" + record[1], 1))\n",
    "        | \"composite Transform HR\" >> myTransform()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53633b56-062a-47ae-9221-4e16342c8822",
   "metadata": {},
   "source": [
    "## CoGroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "821fb100-88b3-4146-bda7-ffd9b5d8d240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retTuple(element):\n",
    "    temp_tuple = element.split(',')\n",
    "    return (temp_tuple[0], temp_tuple[1:]) #pair key, rest of the field\n",
    "with beam.Pipeline() as p13:\n",
    "    dept_rows = (\n",
    "        p13\n",
    "        | \"Read table 1\" >> beam.io.ReadFromText('./testdata/beam_data/cogroupBy/dept_data.txt')\n",
    "        | \"pair key, with rest emp table\" >> beam.Map(retTuple)\n",
    "    )\n",
    "    loc_rows = (\n",
    "        p13\n",
    "        | \"Read table 2\" >> beam.io.ReadFromText('./testdata/beam_data/cogroupBy/location.txt')\n",
    "        | \"pair key, with rest loc table\" >> beam.Map(retTuple)\n",
    "    )\n",
    "    results = ({'dept_data' : dept_rows, 'loc_data' : loc_rows}\n",
    "        | beam.CoGroupByKey()\n",
    "        | beam.io.WriteToText('./testdata/beam_data/cogroupBy/result')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6d3ec8a1-f21a-4679-b27c-e5d9e371d13b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('149633CM', {'dept_data': [['Marco', '10', 'Accounts', '1-01-2019'], ['Marco', '10', 'Accounts', '2-01-2019'], ['Marco', '10', 'Accounts', '3-01-2019'], ['Marco', '10', 'Accounts', '4-01-2019'], ['Marco', '10', 'Accounts', '5-01-2019'], ['Marco', '10', 'Accounts', '6-01-2019'], ['Marco', '10', 'Accounts', '7-01-2019'], ['Marco', '10', 'Accounts', '8-01-2019'], ['Marco', '10', 'Accounts', '9-01-2019'], ['Marco', '10', 'Accounts', '10-01-2019'], ['Marco', '10', 'Accounts', '11-01-2019'], ['Marco', '10', 'Accounts', '12-01-2019'], ['Marco', '10', 'Accounts', '13-01-2019'], ['Marco', '10', 'Accounts', '14-01-2019'], ['Marco', '10', 'Accounts', '15-01-2019'], ['Marco', '10', 'Accounts', '16-01-2019'], ['Marco', '10', 'Accounts', '17-01-2019'], ['Marco', '10', 'Accounts', '18-01-2019'], ['Marco', '10', 'Accounts', '19-01-2019'], ['Marco', '10', 'Accounts', '20-01-2019'], ['Marco', '10', 'Accounts', '21-01-2019'], ['Marco', '10', 'Accounts', '22-01-2019'], ['Marco', '10', 'Accounts', '23-01-2019'], ['Marco', '10', 'Accounts', '24-01-2019'], ['Marco', '10', 'Accounts', '25-01-2019'], ['Marco', '10', 'Accounts', '26-01-2019'], ['Marco', '10', 'Accounts', '27-01-2019'], ['Marco', '10', 'Accounts', '28-01-2019'], ['Marco', '10', 'Accounts', '29-01-2019'], ['Marco', '10', 'Accounts', '30-01-2019'], ['Marco', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9876843261', 'New York'], ['9204232778', 'New York']]})\n",
      "('212539MU', {'dept_data': [['Rebekah', '10', 'Accounts', '1-01-2019'], ['Rebekah', '10', 'Accounts', '2-01-2019'], ['Rebekah', '10', 'Accounts', '3-01-2019'], ['Rebekah', '10', 'Accounts', '4-01-2019'], ['Rebekah', '10', 'Accounts', '5-01-2019'], ['Rebekah', '10', 'Accounts', '6-01-2019'], ['Rebekah', '10', 'Accounts', '7-01-2019'], ['Rebekah', '10', 'Accounts', '8-01-2019'], ['Rebekah', '10', 'Accounts', '9-01-2019'], ['Rebekah', '10', 'Accounts', '10-01-2019'], ['Rebekah', '10', 'Accounts', '11-01-2019'], ['Rebekah', '10', 'Accounts', '12-01-2019'], ['Rebekah', '10', 'Accounts', '13-01-2019'], ['Rebekah', '10', 'Accounts', '14-01-2019'], ['Rebekah', '10', 'Accounts', '15-01-2019'], ['Rebekah', '10', 'Accounts', '16-01-2019'], ['Rebekah', '10', 'Accounts', '17-01-2019'], ['Rebekah', '10', 'Accounts', '18-01-2019'], ['Rebekah', '10', 'Accounts', '19-01-2019'], ['Rebekah', '10', 'Accounts', '20-01-2019'], ['Rebekah', '10', 'Accounts', '21-01-2019'], ['Rebekah', '10', 'Accounts', '22-01-2019'], ['Rebekah', '10', 'Accounts', '23-01-2019'], ['Rebekah', '10', 'Accounts', '24-01-2019'], ['Rebekah', '10', 'Accounts', '25-01-2019'], ['Rebekah', '10', 'Accounts', '26-01-2019'], ['Rebekah', '10', 'Accounts', '27-01-2019'], ['Rebekah', '10', 'Accounts', '28-01-2019'], ['Rebekah', '10', 'Accounts', '29-01-2019'], ['Rebekah', '10', 'Accounts', '30-01-2019'], ['Rebekah', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9995440673', 'Denver']]})\n"
     ]
    }
   ],
   "source": [
    "!powershell -Command \"Get-Content -TotalCount 2 './testdata/beam_data/cogroupBy/result-00000-of-00001'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6befb7e-78f3-4438-ac83-fca3d1a76d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
